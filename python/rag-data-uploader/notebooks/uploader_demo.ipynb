{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_data_uploader import ElasticsearchUploader, OpensearchUploader\n",
    "from rag_data_uploader.utils.mappings import ES7MappingNewCPI, ES7MappingCPI\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:9200\"\n",
    "username = \"\"\n",
    "password = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias = \"rag_data\"\n",
    "folder = \"../data/new_json/nr-ran\"\n",
    "date = \"2023-10-01\"\n",
    "index = f\"{folder.split('/')[-1]}_{date}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we have to make sure that we are running a vector store locally at the url `http://localhost:9200` using one of the docker-compose yaml files in this repo. When this is done, we can instantiate an uploader corresponding to the vector store type we are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploader = ElasticsearchUploader(url, username, password)\n",
    "# uploader = OpensearchUploader(url, username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The uploader has a couple of uploading methods depending on if documents are stored as files or if they are kept in memory. The method which uploads from files does so in batches, to allow for loading large numbers of documents without having to read them all into memory at once. Currently, the maximum number of files uploaded at a time is 10 000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL:  http://localhost:9200\n",
      "Response from document store at URL http://localhost:9200:  OK\n",
      "Some documents had an empty or non-existing content field and were excluded from upload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading documents: 100%|██████████| 7681/7681 [00:02<00:00, 3805.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successfully uploaded documents:  7681\n"
     ]
    }
   ],
   "source": [
    "mapping = ES7MappingNewCPI()\n",
    "uploader.upload_from_folder(folder, index, alias, mapping=mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The method which uploads documents from memory takes similar inputs, and works in a similar way. Let's upload a few documents from the radio folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"../data/new_json/radio\"\n",
    "index = f\"{folder.split('/')[-1]}_{date}\"\n",
    "file_names = [name for name in os.listdir(folder)[:10] if name.endswith(\".json\")]\n",
    "documents_to_upload = []\n",
    "for name in file_names:\n",
    "    with open(f\"{folder}/{name}\", \"r\") as f:\n",
    "        documents_to_upload.extend(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some documents had an empty or non-existing content field and were excluded from upload\n",
      "Number of documents: 379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading documents: 100%|██████████| 379/379 [00:00<00:00, 4625.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successfully uploaded documents:  379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "uploader.upload_documents(documents_to_upload, index=index, alias=alias, mapping=mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we can see that both indices that we have uploaded are included in the specified alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'radio_2023-10-01': {'aliases': {'rag_data': {}}},\n",
       " 'nr-ran_2023-10-01': {'aliases': {'rag_data': {}}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(f\"{url}/_alias/{alias}\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If we upload an index with the same index name but a different date, the uploader will upload the new index, and replace the old index in the alias with the new one. This will allow for version control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some documents had an empty or non-existing content field and were excluded from upload\n",
      "Number of documents: 379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading documents: 100%|██████████| 379/379 [00:00<00:00, 5383.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successfully uploaded documents:  379\n",
      "Index/indices radio_2023-10-01 removed from alias rag_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "date = \"2023-12-01\"\n",
    "index = f\"{folder.split('/')[-1]}_{date}\"\n",
    "uploader.upload_documents(documents_to_upload, index=index, alias=alias, mapping=mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If we again check which indices are in the alias, we see that the old radio index has been replaced with the new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'radio_2023-12-01': {'aliases': {'rag_data': {}}},\n",
       " 'nr-ran_2023-10-01': {'aliases': {'rag_data': {}}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(f\"{url}/_alias/{alias}\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The old index is still stored on the vector store, which will allow us to roll back which data is searchable when we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices in vector store: ['nr-ran_2023-10-01', 'radio_2023-10-01', 'radio_2023-12-01']\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(f\"{url}/*\")\n",
    "indices = [name for name in response.json().keys() if not name.startswith(\".\")]\n",
    "print(\"Indices in vector store:\", indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If we try to upload document to an existing index, with another mapping than the mapping that the index has, the mapping will automatically be changed to the one in the existing index. Let's try to upload documents using a mapping which includes an embedding key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping has been changed to that of alias/index rag_data\n",
      "Some documents had an empty or non-existing content field and were excluded from upload\n",
      "Number of documents: 379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading documents:   0%|          | 0/379 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading documents: 100%|██████████| 379/379 [00:00<00:00, 4021.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successfully uploaded documents:  379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mapping = ES7MappingCPI()\n",
    "uploader.upload_documents(documents_to_upload, index=index, alias=alias, mapping=mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If we try to upload documents to a new index, but to an alias which exists, the uploader will enforce the mapping in the alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-defined mapping is in conflict with alias mapping.\n",
      "Changing mapping to alias mapping.\n",
      "Some documents had an empty or non-existing content field and were excluded from upload\n",
      "Number of documents: 379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading documents: 100%|██████████| 379/379 [00:00<00:00, 2086.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successfully uploaded documents:  379\n",
      "Index/indices radio_2023-12-01 removed from alias rag_data\n"
     ]
    }
   ],
   "source": [
    "date = \"2023-12-15\"\n",
    "index = f\"{folder.split('/')[-1]}_{date}\"\n",
    "uploader.upload_documents(documents_to_upload, index=index, alias=alias, mapping=mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can see the indices in the vector store as well as the alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices in vector store: ['nr-ran_2023-10-01', 'radio_2023-10-01', 'radio_2023-12-01', 'radio_2023-12-15']\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(f\"{url}/*\")\n",
    "indices = [name for name in response.json().keys() if not name.startswith(\".\")]\n",
    "print(\"Indices in vector store:\", indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'radio_2023-12-15': {'aliases': {'rag_data': {}}},\n",
       " 'nr-ran_2023-10-01': {'aliases': {'rag_data': {}}}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(f\"{url}/_alias/{alias}\")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uploader-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
